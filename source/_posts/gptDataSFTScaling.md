---
title: (原理)SFT Scaling
date: 2023-04-26 16:55:39
tags:
  - dataset
categories: 
  - AIGC
  - dataset  
---

<p></p>
<!-- more -->


# 论文
+ 论文地址
 《When Scaling Meets LLM Fine-tuning: The Effect of Data, Model and Fine-tuning Method》


# 摘要[1]
这篇论文研究了大型语言模型（LLMs）的微调（finetuning）问题，尤其是在不同规模因素下的微调性能。作者探讨了包括LLM模型大小、预训练数据大小、新微调参数大小和微调数据大小在内的多个因素，并考虑了两种微调方法：全模型微调（FMT）和参数高效微调（PET，包括prompt tuning和LoRA）。研究发现LLM微调遵循基于**功率的乘法联合规模法则**，**LLM模型规模的增加对微调性能的提升大于预训练数据规模的增加，而PET参数规模的增加通常效果不佳**。此外，**微调方法的选择高度依赖于具体任务和微调数据**。

【 功率的乘法联合规模法则: 微调数据数量 <--> xxx】
【模型大小(标题里的Model ) > 预训练数据(标题里的Data),   PET参数(标题里的Fine-tuning Method) 无效】
【微调方法的选择高度依赖于具体任务和微调数据】

# 实验方法[1]
实验基于两组预训练的双语LLMs（英语&德语，英语&中文），模型大小从1B到16B。作者在WMT机器翻译（英语-德语、英语-中文）和多语言摘要（英语、德语、法语和西班牙语）任务上进行了大规模研究，最多使用20M微调示例。实验设置包括：
- **下游任务**：选择机器翻译和多语言摘要作为微调的下游任务。
- **LLMs和预训练**：采用解码器仅Transformer模型，使用修改后的UL2目标进行训练。
- **微调设置**：研究了三种微调方法（FMT、Prompt和LoRA），并探索了四种不同的规模因素。
- **评估**：使用基于token级别的困惑度（PPL）选择最佳检查点进行评估，并使用BLEURT和RougeL评估生成质量。

# 结论[1]
- 提出了一个乘法联合规模法则来描述微调数据大小和其他规模因素之间的规模关系。
- LLM模型规模的增加对微调性能的提升大于预训练数据规模的增加。
- PET参数规模的增加对于LoRA和Prompt的效果有限，且有时甚至会导致反向规模效应。
- 微调方法的选择对于下游任务来说并不简单，需要根据任务特性和微调数据的可用性来决定。
- 微调可能会提高模型对相关任务的零样本泛化能力，尤其是当基础LLM较大时，Prompt和LoRA通常比FMT表现得更好。

作者指出，尽管研究提供了有价值的见解，但也存在一些局限性，如联合规模法则主要基于封闭生成任务的实证结果，缺乏理论基础。未来的工作将扩展到多模态LLMs，探索微调数据质量的影响，并考虑开放和创造性的生成任务以及微调的多任务设置。


# 重要结论[2]

作者们探讨了大型语言模型（LLMs）在微调（finetuning）过程中不同规模因素对性能的影响。以下是论文的一些重要结论及其对“SCALING”概念的解释：

1. **乘法联合缩放法则**：作者提出了一个基于**乘法的联合缩放法则（multiplicative joint scaling law）**，用于描述微调数据大小与其他缩放因素（如LLM模型大小、预训练数据大小、PET参数大小）之间的关系。这个法则表明，**微调性能与这些因素的乘法组合有关**，而不是简单的加法关系。

2. **模型大小对微调的影响**：研究发现，**增加LLM模型的大小对微调性能的提升比增加预训练数据的大小更为显著**。这表明在有限资源下，**优先考虑扩大模型规模而不是数据规模**，可能会带来更好的微调效果。

3. **参数高效微调（PET）的局限性**：尽管PET方法（如prompt tuning和LoRA）旨在通过优化少量参数来提高性能，但研究发现**增加PET参数的大小对于微调性能的提升效果有限，有时甚至会出现反向缩放现象**。

4. **任务和数据依赖性**：微调的缩放特性高度依赖于具体任务和数据。这意味着**没有一种通用的最优微调方法**，选择哪种微调方法需要根据下游任务的特性和可用的微调数据量来决定。

5. **微调对零样本泛化能力的影响**：尽管微调通常是为了提高特定任务的性能，但研究发现，基于LLM的微调仍然可以促进对相关任务的零样本泛化能力。特别是PET方法在保留模型的泛化能力方面表现更好。

6. **微调数据量的临界点**：论文中还讨论了不同微调方法之间的临界点，即在特定的微调数据量下，一种方法可能比另一种方法表现得更好。这个临界点会随着任务和模型大小的不同而变化。

这些结论对理解LLM微调过程中的“SCALING”具有重要意义。它们揭示了不同规模因素如何相互作用以及它们对微调性能的共同影响，为在实际应用中选择和优化微调策略提供了理论依据。通过这些发现，研究者和实践者可以更好地理解在特定条件下如何有效地缩放和配置他们的模型以获得最佳性能。

# 参考
1. [请帮我读篇论文，详细的写出摘要，实验方法，结论]() kimi
2. [请帮我读篇论文，论文有哪些重要的结论？ 这些结论是如何解释题目中的SCALING的？]() kimi

1xx. [值得一看的大模型预训练数据选择策略总结：兼读20240229大模型进展早报 ](https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648409027&idx=1&sn=4083853fd0bfb1790d8df6b4414b6583)
《When Scaling Meets LLM Finetuning: The Effect of Data， Model and Finetuning Method»