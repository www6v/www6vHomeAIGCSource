---
title: (原理)LIMA, LESS
date: 2023-04-27 17:22:35
tags:
  - dataset
categories: 
  - AIGC
  - dataset  
---

<p></p>
<!-- more -->

## 目录
<!-- toc -->

# LIMA [1][kimi]

LIMA（Less Is More for Alignment）的实验通过一系列设计精良的步骤来探究数据质量、多样性以及数量对模型性能的影响，从而得出了**提高数据质量和增加提示多样性比单纯增加数据量更能提升模型性能的结论**。以下是**实验方法**的关键步骤：

1. **精心策划的微调数据**：LIMA模型在**1000个**精心策划的提示和回复上进行了**微调**，这些数据被设计为模拟真实用户与AI助手的交互。

2. **消融实验**：通过消融实验，研究者们观察了在增加数据量的同时不增加提示多样性时，模型性能的提升是否有限；而在优化数据质量时，性能是否有显著提升。

3. **数据构造**：研究者从Stack Exchange、wikiHow和Pushshift Reddit数据集收集数据，并进行了**质量和多样性**的控制。这些数据集被用来构造训练样本，以确保输入的多样性和输出的一致性。

4. **质量与多样性的对比**：研究者比较了经过质量过滤的Stack Exchange数据和同质化的wikiHow数据对模型性能的影响。结果显示，更**多样化的Stack Exchange数据在性能上优于同质化的wikiHow数据**。

5. **数量的对比**：研究者对从Stack Exchange抽取的指数级增加的训练集进行了测试，发现**训练集的翻倍并没有改善响应质量**，从而说明单纯增加数据量并不一定能提升性能。

6. **质量控制的实验**：研究者还比较了未经过任何质量或风格过滤的Stack Exchange数据集与经过过滤的数据集上训练的模型性能，发现过滤后的数据集上训练的模型性能更优。

7. **人类评估**：为了评估LIMA模型的性能，研究者进行了人类偏好研究，将LIMA的输出与其他几个基线模型的输出进行比较，并让人群工作者选择他们更喜欢的输出。

通过这些实验步骤，LIMA的研究得出了**数据质量和提示多样性对于提升模型性能的重要性远超过单纯增加数据量的结论**。这些发现支持了“浅层对齐假说”，即模型在预训练阶段已经学习到了几乎所有知识和能力，而微调过程主要是学习与人类交互的风格和格式。



# LESS[10][kimi]

LESS（Selecting Influential Data for Targeted Instruction Tuning）的实验方法和相应的结论如下：

### 实验方法：

1. **热身训练（Warmup Training）**：使用LoRA（Low-Rank Adaptation）技术对预训练模型进行热身训练，以适应特定的数据分布。

2. **梯度数据存储（Gradient Data Store）**：构建了一个具有投影低维梯度特征的梯度数据存储，该存储可以重复用于不同的目标任务。

3. **数据选择算法**：利用数据存储和算法选择与体现特定能力的少数示例最相似的训练数据点。?

4. **模型训练**：使用选择的数据子集来训练目标模型。

5. **评估**：在不同的下游任务上评估LESS选择的数据子集的性能，包括MMLU、TYDIQA和BBH数据集。

### 结论：

1. **LESS的有效性**：LESS**在不同的模型中都是有效的**，能够在多个评估数据集上提高性能。

2. **数据子集的性能**：**使用LESS选择的5%的数据通常优于使用完整数据集进行训练的结果**。这表明完整数据集可能包含与特定目标任务无关或有害的数据点。

3. **数据的可转移性**：使用较小模型选择的数据可以提高较大模型和不同模型系列的性能，证明了LESS选择的数据具有高度的可转移性。

4. **与其他方法的比较**：LESS是唯一一致有效的方法，相较于其他基线方法（如随机选择、BM25、DSIR、RDS）表现出更好的性能。

5. **计算成本**：LESS的计算成本较高，但由于其有效性，这一成本是合理的。

6. **定性分析**：LESS选择的数据能够体现预期下游应用所需的推理技能，而不是仅仅基于表面形式线索。

7. **局限性**：LESS需要热身训练阶段，这增加了计算负载。此外，使用补全Token的平均梯度可能导致性能问题。还有，最小化验证损失并不总能提高任务性能，且数据选择中的线性度假设是LESS的一个限制。

总体而言，LESS通过选择与目标任务高度相关的数据点，能够在指令微调中实现高效的性能提升，尽管存在一些局限性和计算成本。



# 参考
### LIMA
1. [大模型微调究竟需要多少数据：从三个现有代表工作看几组结论及一点思考 ](https://mp.weixin.qq.com/s/c50HrOfKOqgqGPVRHf6EpA)
     **指令格式的多样性**
     《LIMA: Less Is More for Alignment》
     《MAYBE ONLY 0.5% DATA IS NEEDED》

1xx. [【论文笔记】LIMA: Less Is More for Alignment](https://blog.csdn.net/jinniulema/article/details/133915276)

### LESS

10. [LESS：仅选择5%有影响力的数据优于全量数据集进行目标指令微调](https://zhuanlan.zhihu.com/p/686007325)



1xx. [LESS 实践：用少量的数据进行目标指令微调](https://zhuanlan.zhihu.com/p/686687923)



