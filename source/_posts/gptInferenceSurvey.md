---
title: (ç»¼è¿°)æ¨ç†ä¼˜åŒ–
date: 2023-08-14 13:23:31
tags:
  - Inference
categories: 
  - AIGC
  - Inference 
---

<p></p>
<!-- more -->


## ç›®å½•
<!-- toc -->


# è®ºæ–‡
+ è®ºæ–‡åœ°å€
 [A Survey on Efficient Inference for Large Language Models](https://arxiv.org/abs/2404.14294) 

+ è®ºæ–‡è§£æ
  [è®ºæ–‡è§£æ](https://candied-skunk-1ca.notion.site/A-Survey-on-Efficient-Inference-for-Large-Language-Models-22145473188e437881bf566241492bea?pvs=4)
  
  
# æ¨ç† [10]
### LLM Algorithmic/Eval Survey
[Efficient Large Language Models: A Survey](https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey)


### LLM Train/Inference Framework[ç³»ç»Ÿå±‚ä¼˜åŒ–]
Megatron-LM
vLLM ***
TensorRT-LLM ***
DeepSpeed-FastGen 2x vLLM?  ***
PETALS

### Continuous/In-flight Batching[ç³»ç»Ÿå±‚ä¼˜åŒ–]
Continuous Batching ***
In-flight Batching

### Weight/Activation Quantize/Compress[æ¨¡å‹å±‚ä¼˜åŒ–]
ZeroQuant
GPTQ
SmoothQuant
AWQ ***
SparQ

### IO/FLOPs-Aware/Sparse Attention [æ¨¡å‹å±‚ä¼˜åŒ–]
MQA ***
FlashAttention ***
GQA

### KV Cache Scheduling/Quantize/Dropping[ç³»ç»Ÿå±‚ä¼˜åŒ–]
PagedAttention *** 
TensorRT-LLM KV Cache FP8

### Prompt/Context Compression[æ•°æ®å±‚ä¼˜åŒ–]
Selective-Context
LLMLingua ***
LongLLMLingua ***

### Long Context Attention/KV Cache Optimization
RingAttention
KVQuant
RAGCache
KCache

### Parallel Decoding/Sampling[ç³»ç»Ÿå±‚ä¼˜åŒ–]
Speculative Sampling ***


# å‚è€ƒ
### Awesome-LLM-Inference
10. [Awesome-LLM-Inference Repo](https://github.com/DefTruth/Awesome-LLM-Inference) git
1xx.  [[Awesome-LLM-Inference]ğŸ”¥ç¬¬ä¸‰æœŸï¼š30ç¯‡ï¼ŒLLMæ¨ç†è®ºæ–‡é›†-500é¡µPDF](https://zhuanlan.zhihu.com/p/669777159)
1xx.  [[Awesome-LLM-Inference]ğŸ”¥ç¬¬äºŒæœŸ: 20ç¯‡ï¼ŒLLMæ¨ç†è®ºæ–‡é›†-300é¡µPDF](https://zhuanlan.zhihu.com/p/658091768)
1xx.  [[LLMæ¨ç†ä¼˜åŒ–]ğŸ”¥100+ç¯‡: å¤§æ¨¡å‹æ¨ç†å„æ–¹å‘æ–°å‘å±•æ•´ç†](https://zhuanlan.zhihu.com/p/693680304) 