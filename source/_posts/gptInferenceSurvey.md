---
title: (ç»¼è¿°)æ¨ç†ä¼˜åŒ–
date: 2023-08-14 13:23:31
tags:
  - Inference
categories: 
  - AIGC
  - Inference 
---

<p></p>
<!-- more -->


## ç›®å½•
<!-- toc -->


# è®ºæ–‡
+ è®ºæ–‡åœ°å€
 [A Survey on Efficient Inference for Large Language Models](https://arxiv.org/abs/2404.14294) 

+ è®ºæ–‡è§£æ
  [è®ºæ–‡è§£æ](https://candied-skunk-1ca.notion.site/A-Survey-on-Efficient-Inference-for-Large-Language-Models-22145473188e437881bf566241492bea?pvs=4)
  
  
# æ¨ç† [10]

### Trending LLM/VLM Topics
+ Mooncake
  Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving
+ MInference
 MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse

### LLM Algorithmic/Eval Survey
+ [Efficient Large Language Models: A Survey](https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey)


### LLM Train/Inference Framework[ç³»ç»Ÿå±‚ä¼˜åŒ–]
+ Megatron-LM
   Training Multi-Billion Parameter Language Models Using Model Parallelism
+  **vLLM**
   Efficient Memory Management for Large Language Model Serving with PagedAttention
+ StreamingLLM
   EFFICIENT STREAMING LANGUAGE MODELS WITH ATTENTION SINKS
+ **TensorRT-LLM**
+ **DeepSpeed-FastGen 2x vLLM?**
  DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference
+ PETALS

+ **Mooncake**
   Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving
+ LMDeploy
  LMDeploy: LMDeploy is a toolkit for compressing, deploying, and serving LLMs
+ MLC-LLM
+ LightLLM
+ llama.cpp
+ flashinfer

### Continuous/In-flight Batching[ç³»ç»Ÿå±‚ä¼˜åŒ–]
+ **Continuous Batching**
+ **In-flight Batching**

### Weight/Activation Quantize/Compress[æ¨¡å‹å±‚ä¼˜åŒ–]
+ ZeroQuant
+ **GPTQ**
+ WINT8/4
+ **SmoothQuant**
+ **AWQ**
+ SparQ
+ W4A8KV4

### IO/FLOPs-Aware/Sparse Attention [æ¨¡å‹å±‚ä¼˜åŒ–]
+ **MQA** 
+ **FlashAttention**
+ **GQA**
+ Flash-Decoding
+ FlashAttention-3

### KV Cache Scheduling/Quantize/Dropping[ç³»ç»Ÿå±‚ä¼˜åŒ–]
+ MQA
+ GQA
+ **PagedAttention**
+ TensorRT-LLM KV Cache FP8
+ RadixAttention
+ **DistKV-LLM**
  Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache(@Alibaba etc)

### Prompt/Context Compression[æ•°æ®å±‚ä¼˜åŒ–]
+ Selective-Context
  Compressing Context to Enhance Inference Efficiency of Large Language Models(@Surrey)
+ **LLMLingua** 
  LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models(@Microsoft)
+ **LongLLMLingua**
  LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression(@Microsoft)

### Long Context Attention/KV Cache Optimization
+ Blockwise Attention
  Blockwise Parallel Transformer for Large Context Models(@UC Berkeley)
+ RingAttention
  Ring Attention with Blockwise Transformers for Near-Infinite Context(@UC Berkeley)
+ KVQuant
+ RAGCache
+ KCache
+ YOCO
  You Only Cache Once: Decoder-Decoder Architectures for Language Models(@Microsoft)

### Early-Exit/Intermediate Layer Decoding

### Parallel Decoding/Sampling[ç³»ç»Ÿå±‚ä¼˜åŒ–]
+ **Speculative Sampling**
  Accelerating Large Language Model Decoding with Speculative Sampling(@DeepMind)

### CPU/Single GPU/FPGA/Mobile Inference 
+ OpenVINO
+ xFasterTransformer

# å‚è€ƒ
### Awesome-LLM-Inference
10. [Awesome-LLM-Inference Repo](https://github.com/DefTruth/Awesome-LLM-Inference) git
1xx.  [[Awesome-LLM-Inference]ğŸ”¥ç¬¬ä¸‰æœŸï¼š30ç¯‡ï¼ŒLLMæ¨ç†è®ºæ–‡é›†-500é¡µPDF](https://zhuanlan.zhihu.com/p/669777159)
1xx.  [[Awesome-LLM-Inference]ğŸ”¥ç¬¬äºŒæœŸ: 20ç¯‡ï¼ŒLLMæ¨ç†è®ºæ–‡é›†-300é¡µPDF](https://zhuanlan.zhihu.com/p/658091768)
1xx.  [[LLMæ¨ç†ä¼˜åŒ–]ğŸ”¥100+ç¯‡: å¤§æ¨¡å‹æ¨ç†å„æ–¹å‘æ–°å‘å±•æ•´ç†](https://zhuanlan.zhihu.com/p/693680304) 