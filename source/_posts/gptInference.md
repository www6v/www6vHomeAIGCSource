---
title: (ç»¼è¿°)æ¨ç†ä¼˜åŒ–
date: 2023-01-01 22:58:43
tags:
  - Inference
categories: 
  - AIGC
  - Inference 
---

<p></p>
<!-- more -->

## ç›®å½•
<!-- toc -->

# æ¨ç† ä¼˜åŒ–
### overview[2]
æœ‰å‡ ç§æ–¹æ³•å¯ä»¥åœ¨å†…å­˜ä¸­**é™ä½æ¨ç†æˆæœ¬**æˆ–/å’Œ**åŠ å¿«æ¨ç†é€Ÿåº¦**ã€‚
- åº”ç”¨å„ç§**å¹¶è¡Œå¤„ç†æ–¹å¼**ï¼Œä»¥åœ¨å¤§é‡GPUä¸Šæ‰©å±•æ¨¡å‹ã€‚æ™ºèƒ½å¹¶è¡Œå¤„ç†æ¨¡å‹ç»„ä»¶å’Œæ•°æ®ä½¿å¾—è¿è¡Œæ‹¥æœ‰æ•°ä¸‡äº¿å‚æ•°çš„æ¨¡å‹æˆä¸ºå¯èƒ½ã€‚
- **å†…å­˜å¸è½½**ï¼Œå°†ä¸´æ—¶æœªä½¿ç”¨çš„æ•°æ®å¸è½½åˆ°CPUï¼Œå¹¶åœ¨ä»¥åéœ€è¦æ—¶å†è¯»å›ã€‚è¿™æœ‰åŠ©äºå‡å°‘å†…å­˜ä½¿ç”¨ï¼Œä½†ä¼šå¯¼è‡´æ›´é«˜çš„å»¶è¿Ÿã€‚
- **æ™ºèƒ½æ‰¹å¤„ç†ç­–ç•¥**ï¼›ä¾‹å¦‚ï¼ŒEffectiveTransformerå°†è¿ç»­çš„åºåˆ—æ‰“åŒ…åœ¨ä¸€èµ·ï¼Œä»¥æ¶ˆé™¤æ‰¹å¤„ç†å†…çš„å¡«å……ã€‚
- **ç½‘ç»œå‹ç¼©æŠ€æœ¯**ï¼Œå¦‚**ä¿®å‰ªã€é‡åŒ–ã€è’¸é¦**ã€‚è¾ƒå°çš„æ¨¡å‹ï¼Œæ— è®ºæ˜¯å‚æ•°æ•°é‡è¿˜æ˜¯ä½å®½ï¼Œåº”è¯¥éœ€è¦æ›´å°‘çš„å†…å­˜å¹¶ä¸”è¿è¡Œæ›´å¿«ã€‚
- é’ˆå¯¹ç›®æ ‡æ¨¡å‹æ¶æ„çš„ç‰¹å®šæ”¹è¿›ã€‚è®¸å¤š**æ¶æ„å˜åŒ–**ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹æ³¨æ„åŠ›å±‚çš„å˜åŒ–ï¼Œæœ‰åŠ©äºæé«˜Transformerè§£ç é€Ÿåº¦ã€‚

### æ¨¡å‹å‹ç¼© [1]

{% asset_img 'compress.png' %}

+ å‰ªæï¼ˆPruningï¼‰
+ çŸ¥è¯†è’¸é¦ï¼ˆKnowledge Distillationï¼ŒKDï¼‰
+ é‡åŒ–ï¼ˆQuantizationï¼‰
+ ä½ç§©åˆ†è§£ï¼ˆLow-Rank Factorizationï¼‰

### KV Cache

# æ¨ç† [10]
### LLM Algorithmic/Eval Survey
[Efficient Large Language Models: A Survey](https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey)


### LLM Train/Inference Framework
Megatron-LM
vLLM ***
TensorRT-LLM ***
DeepSpeed-FastGen 2x vLLM?  ***
PETALS

### Continuous/In-flight Batching
Continuous Batching
In-flight Batching

### Weight/Activation Quantize/Compress
ZeroQuant
GPTQ
SmoothQuant
AWQ
SparQ

### IO/FLOPs-Aware/Sparse Attention
MQA
FlashAttention ***
GQA

### KV Cache Scheduling/Quantize/Dropping
PagedAttention *** 
TensorRT-LLM KV Cache FP8

### Prompt/Context Compression
Selective-Context
LLMLingua ***
LongLLMLingua ***

### Long Context Attention/KV Cache Optimization
RingAttention
KVQuant
RAGCache
KCache

### Parallel Decoding/Sampling
Speculative Sampling ***

# å‚è€ƒ
### ç»¼è¿°
1. [ä¸€æ–‡æ¢ç§˜LLMåº”ç”¨å¼€å‘(13)-æ¨¡å‹éƒ¨ç½²ä¸æ¨ç†(ä¼˜åŒ–ç†è®º) ](https://mp.weixin.qq.com/s/glPPSqHjsnDjC0DZSuuPzA) 

2. [Large Transformer Model Inference Optimization ](https://lilianweng.github.io/posts/2023-01-10-inference-optimization/)

1xx. [å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ€§èƒ½ä¼˜åŒ–ç»¼è¿°](https://zhuanlan.zhihu.com/p/656485997)

1xx. [NLPï¼ˆåå…«ï¼‰ï¼šLLM çš„æ¨ç†ä¼˜åŒ–æŠ€æœ¯çºµè§ˆ](https://zhuanlan.zhihu.com/p/642412124) *** 

1xx. [3ä¸‡å­—è¯¦ç»†è§£ææ¸…åå¤§å­¦æœ€æ–°ç»¼è¿°å·¥ä½œï¼šå¤§æ¨¡å‹é«˜æ•ˆæ¨ç†ç»¼è¿°](https://mp.weixin.qq.com/s/U9ESiWehnoKc9SnDz7DVKg)

### Awesome-LLM-Inference
10. [Awesome-LLM-Inference Repo](https://github.com/DefTruth/Awesome-LLM-Inference) git
1xx. [[Awesome-LLM-Inference]ğŸ”¥ç¬¬ä¸‰æœŸï¼š30ç¯‡ï¼ŒLLMæ¨ç†è®ºæ–‡é›†-500é¡µPDF](https://zhuanlan.zhihu.com/p/669777159)
1xx. [[Awesome-LLM-Inference]ğŸ”¥ç¬¬äºŒæœŸ: 20ç¯‡ï¼ŒLLMæ¨ç†è®ºæ–‡é›†-300é¡µPDF](https://zhuanlan.zhihu.com/p/658091768)
1xx. [[LLMæ¨ç†ä¼˜åŒ–]ğŸ”¥100+ç¯‡: å¤§æ¨¡å‹æ¨ç†å„æ–¹å‘æ–°å‘å±•æ•´ç†](https://zhuanlan.zhihu.com/p/693680304)

