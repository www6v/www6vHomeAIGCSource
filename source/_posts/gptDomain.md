---
title: 垂类大模型
date: 2023-01-04 10:13:10
tags:
  - 大模型
categories: 
  - AIGC
  - 大模型  
---

<p></p>
<!-- more -->

## 目录
<!-- toc -->

# Domain Specialization [0]
{% asset_img 'domain.JPG' %}

LLM（Large Language Models）的领域专业化可以理解为将广泛训练的通用LLM调整到特定领域内以实现最佳操作。为了应对第1节中提到的领域专业化的三个挑战，LLM领域专业化的方法可以分为三类：外部增强、提示设计和模型微调。这些类别对应了对LLM的不同访问级别的假设，即无访问（黑盒）、部分访问（灰盒）和完全访问（白盒）。黑盒假设通常表示我们只能访问模型API（例如ChatGPT），除了生成的输出之外不知道任何信息；灰盒假设表示我们有限的信息（例如GPT-3 API中生成的标记的概率），这些信息可以指导我们设计和微调一个合适的提示，以更好地引出领域知识；白盒假设表示我们完全可以访问LLM（例如LLaMA及其变体），包括参数设置、训练数据和模型架构。

除了基于LLM可访问性的分类法之外，根据使用的训练策略，可以将LLM领域专业化方法分类为以下几种：使用领域特定数据对现有模型进行微调，从头开始为特定领域训练模型，或者使用混合训练策略。另一个分类法可以基于干预级别：预训练干预涉及修改预训练过程以鼓励领域特定知识，微调干预涉及在微调阶段进行调整，推理时干预涉及修改模型在实际应用中的行为以生成更多领域特定的输出。此外，也可以基于评估和反馈机制来建立分类法：固定评估设置了一个恒定的基准，动态评估涉及使用不断变化的基准进行持续性能评估，基于用户反馈的评估使用直接用户输入作为调整模型响应的信号。

在这份调查中，我们根据LLM的可访问性对现有方法进行分类，并在图2中概述了每种方法。具体来说，1）外部增强（黑盒）不一定需要访问LLM的内部参数空间，使得对资源有限的用户（如计算资源、领域特定数据）最易于访问。如图2（b）所示，通过使用外部资源或工具，将领域特定知识合并到输入提示、生成的输出或两者中，有效地调整LLM的性能而不修改其内部结构。2）提示设计（灰盒）通过访问LLM的梯度或损失值设计各种类型的提示，从而对模型的行为进行更精细的控制。3）模型微调（白盒）需要最多的访问和资源，因为它涉及更新LLM的参数，直接将领域特定知识纳入模型中（图2（d））。

不同类别方法之间的关系：
• 不同的专业化水平：每种方法在不同的专业化水平上操作（即黑盒、灰盒和白盒）。使用外部知识进行增强提供了领域特定信息的集中注入，而提示工程则在输入层面上塑造模型的推理过程。模型微调修改了LLM的内部参数，导致模型行为发生更深刻的变化。

• 权衡：这些方法在计算成本、实施便利性和泛化能力方面存在不同的权衡。使用外部信息进行增强和设计特定任务的指令通常比LLM的知识更新计算成本低，但可能无法达到相同水平的性能改进。模型微调和神经适配器可以提供更实质性的性能提升，但在实施上可能更具挑战性，并且如果出现过拟合，可能会导致泛化能力降低。

• 互补性：这三种方法可以独立使用或组合使用，以在领域特定任务上实现更好的性能。例如，可以将外部知识与经过微调的LLM结合起来，充分利用专业化知识和优化的参数。同样，精心设计的提示可以与神经适配器一起使用，引导模型的输出同时利用新学习的领域特定知识。


### 总结 [chatmind]
- LLM（Large Language Models）领域专业化
  - 领域专业化的定义
    - 将广泛训练的通用LLM调整到特定领域内以实现最佳操作
  - 领域专业化的挑战
    - 挑战一：领域专业化的需求
    - 挑战二：领域专业化的数据稀缺性
    - 挑战三：领域专业化的知识缺乏
  - LLM领域专业化的方法
    - 方法一：**外部增强**  黑盒 
      - 利用外部资源来增强LLM的领域专业化
      - 例如，使用特定领域的数据集进行预训练
    - 方法二：**提示设计** 灰盒
      - 设计适当的提示来引导LLM生成符合特定领域需求的内容
      - 根据领域知识和需求，精心设计提示
    - 方法三：**模型微调** 白盒
      - 对预训练的LLM进行微调，使其更适应特定领域的任务和数据
      - 通过在特定领域的数据集上进行迭代训练，优化模型性能
  - LLM访问级别的假设
    - **无访问（黑盒）**假设
      - 只能访问模型API，无法获取其他信息
      - 仅通过生成的输出来进行操作
    - **部分访问（灰盒）**假设
      - 拥有有限的信息，如生成的标记概率
      - 可根据这些信息设计提示来引导LLM生成符合领域知识的内容
    - **完全访问（白盒）**假设
      - 可以完全访问LLM，获取其所有信息
      - 可以根据需要进行修改和优化
    - 不同访问级别的假设对应不同的方法和操作方式


# 领域微调模型 [1]

+ 注入领域知识，分成三种：
  - 继续预训练注入
  - 微调注入以及
  - 外挂注入

# 关注点 [2]
+ **领域相关数据**  是Continue PreTrain的关键
+ **混合通用数据**以**缓解模型遗忘通用能力**
+ **领域模型Continue PreTrain**时可以同步加入**SFT数据**，即MIP，Multi-Task Instruction PreTraining
+ 仅用SFT做领域模型时，**资源有限**就用在**Chat模型基础上训练**，**资源充足**就在**Base模型上训练**。（**资源=数据+显卡**）
+在Chat模型上进行SFT时，请一定**遵循Chat模型原有的系统指令&数据输入格式**。
+ 领域评测集时必要内容，建议有两份，一份选择题形式自动评测、一份开放形式人工评测。


# 参考
0. 《Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey》
1. [领域微调大模型入局的自我和解：领域微调大模型若一定要做，则务必想的若干个前提条件 ](https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648401405&idx=1&sn=59baf4a22d9a9abeb42599ac91e11a79)
2. [领域大模型-训练Trick&落地思考](https://zhuanlan.zhihu.com/p/648798461)


1xx. [垂直领域大模型的一些思考及开源模型汇总](https://zhuanlan.zhihu.com/p/642611747)
1xx. [层出不穷的垂域微调大模型非最全汇总：12大领域、57个领域微调模型概述及对垂直行业问答的一些讨论 ](https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648403459&idx=2&sn=0219fc098c208e36cd32940e71089fd2) 领域模型集合
    [Awesome-Domain-LLM](https://github.com/www6v/Awesome-Domain-LLM)
1xx. [医疗金融法律大模型：从ChatDoctor到BloombergGPT/FinGPT/FinBERT、ChatLaw/LawGPT_zh](https://blog.csdn.net/v_JULY_v/article/details/131550529?spm=1001.2014.3001.5502)
1xx. [再谈垂直领域大模型及今日前沿速递：金融领域FinBERT、BloombergGPT以及法律领域微调模型LawGPT_zh](https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&mid=2648400666&idx=1&sn=bc47e8c4eca6fc4baaded42fa3c6bd77)


1xx. {% post_link 'gptLeaderBoard' %} self
1xx. [Awesome-Domain-LLM](https://github.com/www6v/Awesome-Domain-LLM)  git 全
